{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Joins\n",
    "\n",
    "One of the best ways to combine two dataframes together is by joining them. The join method can be used in many different ways.\n",
    "\n",
    "### Types of joins\n",
    "\n",
    "There are several ways you can join a dataframe each way changes how your dataframe will look and function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import concat, lit\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataframe to Work With"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_data = [\n",
    "    (\"Luke Skywalker\", \"Rebel\"),\n",
    "    (\"Darth Vader\", \"Empire\"),\n",
    "    (\"Boba Fett\", \"Bounty Hunter\")\n",
    "]\n",
    "\n",
    "names = spark.createDataFrame(names_data, [\"name\", \"faction\"])\n",
    "\n",
    "names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factions_data = [\n",
    "    (\"Jawas\", \"Neutral Evil\"),\n",
    "    (\"Rebel\", \"Chaotic Good\"),\n",
    "    (\"Empire\", \"Lawful Evil\")\n",
    "]\n",
    "\n",
    "factions = spark.createDataFrame(factions_data, [\"faction\", \"alignment\"])\n",
    "\n",
    "factions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Join / Left Outer\n",
    "\n",
    "Left join is the first join that joins the right dataframe to the left, based on the column provided. Anything on the left that's not on the right is nulled. Anything on the right that's not on the left is not joined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.show()\n",
    "factions.show()\n",
    "\n",
    "left_join = names.join(factions, on='faction', how='left')\n",
    "left_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right / Right Outer\n",
    "\n",
    "Joins the left dataframe to the right, based on the column provided. Anything on the right that's not on the left is nulled. Anything on the left that's not on the right is not joined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.show()\n",
    "factions.show()\n",
    "\n",
    "right_join = names.join(factions, on='faction', how='right')\n",
    "right_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer / Full Outer / Full\n",
    "Joins both dataframe, filling the dataframe with null wherever the data don't align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.show()\n",
    "factions.show()\n",
    "\n",
    "outer_join = names.join(factions, on='faction', how='outer')\n",
    "outer_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_join.show()\n",
    "right_join.show()\n",
    "outer_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Inner, Semi, Cross and Anti Joins\n",
    "\n",
    "First you will need dataframes to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "# need to import for working with pandas\n",
    "import pandas as pd\n",
    "# need to import to use pyspark\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=1, c=3, d=1,\n",
    "        e=1),\n",
    "    Row(a=2, b=2, c=1, d=5,\n",
    "        e=2),\n",
    "    Row(a=4, b=5, c=7, d=12,\n",
    "        e=3)\n",
    "])\n",
    "df2 = spark.createDataFrame([\n",
    "    Row(f=1, g=2, h=5, i=2,\n",
    "        j=2),\n",
    "    Row(f=5, g=3, h=3, i=6,\n",
    "        j=3),\n",
    "    Row(f=4, g=6, h=8, i=12,\n",
    "        j=4)\n",
    "])\n",
    "# show table\n",
    "df.show()\n",
    "df2.show()\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"df1\")\n",
    "df2.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"df2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join\n",
    "Method to join two tables taking only entries that have the same key in both. Returns a dataframe with columns from both tables that match on a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(\"SELECT * FROM df1 INNER JOIN df2 ON df1.a=df2.g\"))\n",
    "display(spark.sql(\"SELECT * FROM df1 INNER JOIN df2 ON df1.a=df2.f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi Join\n",
    "Method to join two tables taking only entries that have the same key in both. Returns a Dataframe with columns from one table that has a key match in the other table. Opposite of Anti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(\"SELECT * FROM df1 LEFT SEMI JOIN df2 ON df1.a=df2.g\"))\n",
    "display(spark.sql(\"SELECT * FROM df1 LEFT SEMI JOIN df2 ON df1.a=df2.f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anti Join\n",
    "Method to join two tables taking only entries that do not have the same key in both. Returns a Dataframe with columns from one table that does not have a key match in the other table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(\"SELECT * FROM df1 LEFT ANTI JOIN df2 ON df1.a=df2.g\"))\n",
    "display(spark.sql(\"SELECT * FROM df1 LEFT ANTI JOIN df2 ON df1.a=df2.f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Join\n",
    "Joins two tables by taking every possible combination of entries. Returns the cartesian product of two tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(\"SELECT * FROM df1 CROSS JOIN df2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Concat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'county': 'Clark County', 'state': 'Nevada', 'crime_rate': 0.5},\n",
    "    {'county': 'Madison County', 'state': 'Idaho', 'crime_rate': 0.2},\n",
    "    {'county': 'Yuma County', 'state': 'Colorado', 'crime_rate': 0.05}\n",
    "]\n",
    "df_1 = pd.DataFrame(data)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'county': 'Fairfax County', 'state': 'Virginia', 'crime_rate': 0.02},\n",
    "    {'county': 'Bergen County', 'state': 'New Jersey', 'crime_rate': 0.06},\n",
    "    {'county': 'Los Alamos County', 'state': 'New Mexico', 'crime_rate': 0.1}\n",
    "]\n",
    "df_2 = pd.DataFrame(data)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df_1, df_2], ignore_index=True, axis=0)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark Concat Function Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    Row(county='Clark County', state='Nevada', crime_rate=0.5),\n",
    "    Row(county='Madison County', state='Idaho', crime_rate=0.2),\n",
    "    Row(county='Yuma County', state='Colorado', crime_rate=0.05)\n",
    "])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame([\n",
    "    Row(county='Fairfax County', state='Virginia', crime_rate=0.02),\n",
    "    Row(county='Bergen County', state='New Jersey', crime_rate=0.06),\n",
    "    Row(county='Los Alamos County', state='New Mexico', crime_rate=0.1)\n",
    "])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = df1.withColumn('location', concat(df1.county, lit(', '), df1.state))\n",
    "# df1 = df1.drop('county', 'state')\n",
    "concat_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack Dataframes with Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.createDataFrame([\n",
    "    Row(county='Jefferson County', state='Idaho', crime_rate=0.08, fips=200)\n",
    "])\n",
    "df4 = spark.createDataFrame([\n",
    "    Row(crime_rate=0.08, county='Jefferson County', state='Idaho')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = df1.union(df2)\n",
    "combined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swapped Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = df1.union(df4)\n",
    "combined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = df1.unionByName(df3)\n",
    "combined.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
